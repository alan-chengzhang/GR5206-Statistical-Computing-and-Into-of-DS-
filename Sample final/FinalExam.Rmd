---
title: "GU4206-GR5206 Final Exam [100 pts]"
author: "Student (UNI)"
date: "May 4th, 2018"
output:
  word_document: default
  pdf_document: default
---



The STAT Spring 2018 GU4206-GR5206 Final Exam is open notes, open book(s), open computer and online resources are allowed.  Students are **not** allowed to communicate with any other people regarding the final with the exception of the instructor (Gabriel Young) and TA (Fan Gao).  This includes emailing fellow students, using WeChat and other similar forms of communication.  If there is any suspicious of students cheating, further investigation will take place.  If students do not follow the guidelines, they will receive a zero on the exam and potentially face more severe consequences.  The exam will be posted on Canvas at 9:00AM.  Students are required to submit both the .pdf and .Rmd files on Canvas (or .html if you must) by 12:00PM. Late exams will not be accepted. If for some reason you are unable to upload the completed exam on Canvas by 12:00PM, them immediately email the markdown file to the course TA.           


# Part 1: Simulation [40 pts]  

In this section, we consider a **mixture** of two normal distributions.  Here we assume that our random variable is governed by the probability density $f(x)$, defined by
\begin{align*}
f(x)&=f(x;\mu_1,\sigma_1,\mu_2,\sigma_2,\delta)\\
&=\delta f_1(x;\mu_1,\sigma_1)+(1-\delta)f_2(x;\mu_2,\sigma_2)\\
 &=\delta \frac{1}{\sqrt{2 \pi \sigma_1^2}}\exp{-\frac{1}{2\sigma_1^2}(x-\mu_1)^2}+(1-\delta) \frac{1}{\sqrt{2 \pi \sigma_2^2}}\exp{-\frac{1}{2\sigma_2^2}(x-\mu_2)^2}, 
\end{align*}
where  $-\infty<x<\infty$ and the parameter space is defined by $-\infty < \mu_1,\mu_2 <\infty$, $\sigma_1,\sigma_2 >0$, and $0\leq\delta\leq1$.   The **mixture parameter** $\delta$ governs how much mass gets placed on the first distribution $f(x;\mu_1,\sigma_1)$ and the complement of $\delta$ governs how much mass gets placed on the other distribution $f_2(x;\mu_2,\sigma_2)$.  

In our setting, suppose that we are simulating $n=10,000$ heights from the population of both males and females.  Assume that males are distributed normal with mean $\mu_1=70\text{[in]}$ and standard deviation $\sigma_1=3\text{[in]}$ and females are distributed normal with mean $\mu_2=64\text{[in]}$ and standard deviation $\sigma_2=2.5\text{[in]}$.  Also assume that each distribution contributes equal mass, i.e., set the mixture parameter to $\delta=.5$.  The distribution of males is governed by 
\[
f_1(x;\mu_1,\sigma_1)=\frac{1}{\sqrt{2 \pi \sigma_1^2}}\exp{-\frac{1}{2\sigma_1^2}(x-\mu_1)^2}, \ \ \ -\infty<x<\infty,
\]
and the distribution of females is governed by
\[
f_2(x;\mu_2,\sigma_2)=\frac{1}{\sqrt{2 \pi \sigma_2^2}}\exp{-\frac{1}{2\sigma_2^2}(x-\mu_2)^2}, \ \ \ -\infty<x<\infty.
\]
The goal is to **simulate** from the **mixture distribution** 
\[
\delta f_1(x;\mu_1,\sigma_1)+(1-\delta)f_2(x;\mu_2,\sigma_2),
\]
where $\mu_1=70,\sigma_1=3,\mu_2=64,\sigma_2=2.5,\delta=2.5$ using the accept-reject algorithm. 

## Perform the following tasks:

1) [10 pts]   Using **ggplot**, graph $f_1(x;\mu_1,\sigma_1)$, $f_2(x;\mu_2,\sigma_2)$ and the mixture $f(x)$ all on the same plot. Make sure the plot includes a legend and is labeled appropriately.    

```{r}
# Solution goes here 

library(ggplot2)
x <- seq(30,100,by=0.1)
f1 <- dnorm(x,mean=70,sd=3)

f2 <- dnorm(x,mean=64,sd=2.5)

# 目标函数f要写成函数    因为后续会用到
f <- function(x){
  0.5*dnorm(x,mean=70,sd=3)+ 0.5*dnorm(x,mean=64,sd=2.5)
}



# ggplot绘图的简便方法：
# 1、数据准备：把x和所有y直接写入一个dataframe中，再把列名写为图例中想要的列名
# 2、绘图：分开来画，每一个col都放在aes()内，且一列名命名
new <- data.frame(x,f1,f2,f(x))
colnames(new) = c("x","male","female","mixture")
ggplot(data = new) + 
  geom_line(mapping = aes(x=x, y = male, col = "male")) + 
  geom_line(mapping = aes(x=x, y = female,col = "female")) + 
  geom_line(mapping = aes(x=x, y = mixture,col = "mixture"))+
  labs(title = "Plot of three functions", x = "x", y = "y")



#ggolot画图准备工作：先把数据都写到dataframe中，x和name需要以重复形式给出--------
data <- data.frame(x=c(x,x,x),
                   y=c(f1,f2,f(x)),
                   name = c(rep("male",length(x)),rep("female",length(x)),rep("mixed",length(x))))
# 图例的名字即为这里第三列的列名


library(ggplot2)
ggplot(data = data) +
  geom_line(mapping = aes(x = x, y = y, col= name)) +    
  labs(title = "Plot of three functions", x = "x", y = "y")


```

2) [25 pts] Use the **accept-reject** algorithm to simulate from this mixture distribution.  To receive full credit:
\begin{itemize}
\item[2.i] Clearly identify an \textbf{easy to simulate} distribution $g(x)$.  I recommend picking a normal distribution or a Cauchy distribution for $g(x)$. 
\item[2.ii] Identify a \textbf{suitable} value of $alpha$ such that your envelope function $e(x)$ satisfies
\[
f(x) \leq e(x) = g(x)/\alpha, \ \ \text{where} \ \ 0<\alpha<1.
\]
Note that you must choose $\alpha$ so that $e(x)$ is close to $f(x)$.  Show that your $alpha$ is \textbf{suitable} using a plot.
\item[2.iii] Simulate 10,000 draws from the mixture distribution using the \textbf{accept-reject} algorithm.  Display the first 20 simulated values.  Also, using \textbf{ggplot} or \textbf{base R}, construct a histogram of the simulated mixture distribution with the true mixture pdf $f(x)$ overlayed on the plot.
\end{itemize}

2.i) [5 pts]  

```{r}
# Solution goes here 
g <- function(x){
  result <- dnorm(x,mean=65,sd=5)     #通过后面的绘图，来调整mean和sd
  return(result)
}

# 找最大值的方法：
x[which.max(f(x))]
f(x[which.max(f(x))])

```

2.ii) [10 pts]  By inspection, we see that choosing $\alpha=.44$ allows $e(x)$ to be greater than $f(x)$ for all $x$ and the envelope sits relatively close to the target distribution.

```{r}
# Solution goes here 

# alpha要写死 或者给出默认值
e <- function(x,alpha=0.44){
  ex <- g(x)/alpha
  return(ex)
}

x1 <- seq(40,90,0.01)

plot(x1,f(x1),col="blue",type="l",ylim = c(0,1))
lines(x1,e(x1,0.44),col="red")


#-----------用ggplot2 画图
x.vec <-seq(30,100,by=.1)
ggplot()+
  geom_line(mapping =aes(x = x.vec, y =f(x.vec)),col="purple")+
  geom_line(mapping =aes(x = x.vec, y =e(x.vec)),col="green")

```

2.iii) [10 pts] The **Accept-Reject** algorithm is coded below: 

```{r}
# Solution goes here 

n.samps <- 10000   # Samples desired
n       <- 0      # counter for number samples accepted
samps   <- numeric(n.samps) # initialize the vector of output
while (n < n.samps) {
  y <- rnorm(1,65,5)   # random draw from g
  u <- runif(1)                     
  if (u < f(y)/e(y)) {
    n        <- n + 1
    samps[n] <- y
  }
}

head(samps)

```

Plot: 

```{r}
# Solution goes here 

hist(samps,breaks=50,probability = TRUE,main="Accept-Reject",xlim=c(40,90))
lines(x1,f(x1),col="blue")



#---------------ggolot2--------------
mix.sim <-data.frame(mix=samps)
ggplot(mix.sim)+
  geom_histogram(mapping=aes(x=mix,y=..density..),col="blue",bins=50)+
  xlim(45,90)+      #对x加上限制条件
  stat_function(fun=f,colour="green")

```

3) [5 pts] Slightly change the **Accept-Reject** algorithm from Part (2.iii) to also include the acceptance rate, i.e., how many times did the algorithm accept a draw compared to the total number of trials performed.  What proportion of cases were accepted?  Compare this number to your chosen $\alpha$ and comment on the result.

```{r}
# Solution goes here 


f <- function(x){
  0.5*dnorm(x,mean=70,sd=3)+ 0.5*dnorm(x,mean=64,sd=2.5)
}

# Accept-Reject中，n为接受的数量（因为他已经是写在if后面了）


n.samps <- 10000   # Samples desired
n       <- 0      # counter for number samples accepted
total_n <- 0
samps   <- numeric(n.samps) # initialize the vector of output
while (n < n.samps) {
  total_n <- total_n + 1
  y <- rnorm(1,65,5)      # random draw from g
  u <- runif(1)                     
  if (u < f(y)/e(y)) {
    n        <- n + 1
    samps[n] <- y
  }
}

acceptance_rate <- n.samps/total_n
acceptance_rate

```
# it is really close to alpha




# Part II: Maximum Likelihood Estimaiton and Newton's Method [25 pts] 

Recall in logistic regression, the likelihood function is derived by **linking** the mean of $Y_i$ with a linear function.

**Logistic Regression Model:**

Let $Y_1,Y_2,\ldots,Y_n$ be independently distributed Bernoulli random variables with respective success probabilities $p_1,p_2,\ldots,p_n$.  Then the **logistic regression model** is: 
\[
E[Y_i]=p_i=\frac{e^{(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{i,p})}}{1+e^{(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{i,p})}}, \ \ \ \, i=1,2,\ldots,n.
\]

Notice with some simple algebra, the above model can be expresses as:

\[
\log \bigg{(} \frac{p_i}{1-p_i}\bigg{)}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{i,p} x_i, \ \ \ \, i=1,2,\ldots,n.
\]
The main idea is to **link** the expected value of $Y_i$ ($E[Y_i]=p_i$) to a linear function. This same principle can be applied to other settings. 

## Data Description

Consider a geriatrics experiment designed as a prospective study to investigate the effects of two interventions on the frequency of falls. One hundred subjects were randomly assigned to one of the two interventions: education only ($X_1 = 0$) and education plus aerobic exercise training ($X_1 = 1$). Subjects were at least 65 years of age and in reasonably good health. Three variables considered to be important as control variables were gender ($X_2:0=$female, 1=male), a balance index ($X_3$), and a strength index ($X_4$). The higher the balance index, the more stable the subject: and the higher the strength index, the stronger the subject. Let $Y$ be the number of falls during the six month study.

```{r}
setwd("C:/Users/Alan_/Desktop/Semester1 Courses/STAT computing/sample final")
glm.data <- read.table("CH14PR39.txt")
names(glm.data) <- c("Y","X1","X2","X3","X4")
head(glm.data)
```

In this setting, the response variable takes on discrete count values, therefore it is reasonable to assume $Y_1,Y_2,\ldots,Y_{100}$ are independent Poisson random variables with mean $E[Y_i]=\lambda_i$.  Here we can choose a link function that relates $\lambda_i$ to the linear function $\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i3}+\beta_4x_{i4}$.  

**Perform the follwoing task:**

4) [25 pts] Assume $Y_1,Y_2,\ldots,Y_{100}$ are independent Poisson random variables with mean
\[
E[Y_i]=\lambda_i=\exp{(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i3}+\beta_4x_{i4})}
\]
Note that the link function is $exp(u)$.  Use maximum likelihood estimation to estimate the Poisson regression model.  To receive full credit:

\begin{itemize}
\item[4.i] Define the negative log-likelihood function in R.  Name the function \textbf{pois.neg.ll}.
\item[4.ii] Test the negative log-likelihood function at the parameter point \textbf{rep(0,5)}.
\item[4.iii] Use the \textbf{Newton's Method} or \textbf{Gradient Descent} algorithm from class to estimate coefficients $\beta_0,\beta_1,\beta_2,\beta_3,\beta_4$. Display the estimated parameters and the number of iterations the algorithm took to converge.  For partial credit, you can use \textbf{nlm()}.      
\end{itemize}

4.i) [10 pts]  
```{r}
# Solution goes here
# 求似然函数：doo(Y的数据、参数、log=TRUE)

pois.neg.ll <- function(beta){
  lambda <- exp(beta[1]+
    glm.data$X1*beta[2]+
    glm.data$X2*beta[3]+
    glm.data$X3*beta[4]+
    glm.data$X4*beta[5])
    neg.ll <- -sum(dpois(glm.data$Y, lambda, log = TRUE))
    return(neg.ll)
}

```

4.ii) [5 pts]  

```{r}
# Solution goes here 
pois.neg.ll(beta=rep(0,5))

```

4.iii) [10 pts]   
```{r}
# Solution goes here 
library(numDeriv)
Newtons.Method <- function(f, x0, max.iter = 200, stopping.deriv = 0.01, ...) {
 
  n <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[,1] <- x0
  for (k in 2:max.iter) {
    # Calculate the gradient
    grad.cur <- grad(f, xmat[ ,k-1], ...)
    # the hessian    use hessian to be the step_length of each step
    hess.cur <- hessian(f, xmat[, k-1], ...)
    # Should we stop?
    if (all(abs(grad.cur) < stopping.deriv)) {
      k <- k-1
      break
    }
    # Move in the opposite direction of the grad    solve():get inverse of matrix
    xmat[ ,k] <- xmat[ ,k-1] - solve(hess.cur) %*% grad.cur
  }
  
  xmat <- xmat[ ,1:k] # Trim
  return(list(x = xmat[,k], xmat = xmat, k = k, minimum=f(xmat[,k],...)))
}

out <- Newtons.Method(pois.neg.ll, x0 = rep(0,5)) 
out$x
out$k

#----------------------------------------------------------------------


```




```{r}
# 如果数据集是写死的，nlm(函数、初始参数)即可
nlm(pois.neg.ll,p=rep(0,5))

# 如果数据集没有写死，则后面还要加上数据参量（数据为整个数据集）
# nlm(neg.gamma.ll, c(19, 1), data = cats$Hwt)

```



Check the result with **glm()** (optional)

```{r}
glm(Y~X1+X2+X3+X4,data=glm.data,family="poisson")
```
##### These two results are almost the same.



# Part III: Cross Validadtion and Linear Regression [35 pts] 

The goal of this section is to illustrate how the **degree** of a polynomial model can be thought of as a tuning parameter.  

**Perform the follwoing task:**


5) [5 pts] Upload the dataset **finalexamtrain.csv** and plot $Y$ versus $x$ using **ggplot**.  Change the points to blue in the plot.   

```{r}
train <- read.csv("finalexamtrain.csv")

ggplot(data = train) +
  geom_point(mapping = aes(x = x, y = Y),col="blue") +
  labs(title = "train data Plot", x = "x", y = "Y")

```

6) [5 pts] In this setting, we regress the response variable $Y$ against a single predictor $x$ using five different models:

**Linear Model: degree=1**
\[
Y_i=\beta_0+\beta_0 x_i+\epsilon_i, \ \ \epsilon_i \sim N(0,\sigma^2) 
\]
**Quadratic Model: degree=2**
\[
Y_i=\beta_0+\beta_1 x_i+\beta_2 x_i^2+\epsilon_i, \ \ \epsilon_i \sim N(0,\sigma^2) 
\]
**Cubic Model: degree=3**
\[
Y_i=\beta_0+\beta_1 x_i+\beta_2 x_i^2+\beta_3 x_i^3+\epsilon_i, \ \ \epsilon_i \sim N(0,\sigma^2) 
\]
**Quartic Model: degree=4**
\[
Y_i=\beta_0+\beta_1 x_i+\beta_2 x_i^2+\beta_3 x_i^3+\beta_4 x_i^4+\epsilon_i, \ \ \epsilon_i \sim N(0,\sigma^2) 
\]
**Quintic Model: degree=5**
\[
Y_i=\beta_0+\beta_1 x_i+\beta_2 x_i^2+\beta_3 x_i^3+\beta_4 x_i^4+\beta_5 x_i^5+\epsilon_i, \ \ \epsilon_i \sim N(0,\sigma^2)
\]

Fit each model using the training data.  To make your life easier, I recommend using the Inhibit Interpretation function **I()**.  Display the estimated coefficients for each model.   

```{r}
# Solution goes here 
Linear <- lm(Y~x, data = train)
Quadratic <- lm(Y~x+I(x^2), data = train)
Cubic <- lm(Y~x+I(x^2)+I(x^3), data = train)
Quartic <- lm(Y~x+I(x^2)+I(x^3)+I(x^4), data = train)
Quintic <- lm(Y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5), data = train)


## model.quad$coefficients


```

7)  [10 pts]  Consider the dataframe **finalexamtest.csv** that contains four validation (test) sets each consisting of 50 cases.  Notice that the variable **ValSet** contains four levels.  More specifically, the first 50 cases belong to **TestSet1**, the next 50 cases belong to **TestSet2**, ...etc.   
```{r}
data.test <- read.csv("finalexamtest.csv")
head(data.test)
tail(data.test)
levels(data.test$ValSet)
```

Write a function named **test.error** that inputs a dataframe and outputs the **test error** for each model.  The function should give four predictions errors corresponding to each model, i.e., linear, quadratic, cubic, quartic, and quintic. Note that you are still using the trained models from Part (6).  You are allowed to hard code some of this function.  Test the **test.error* function on the full training data. 

**Hint: y.test <- predict(model,newdata = data.test["x"])**

```{r}
# Solution goes here 
test.error <- function(data=train){
  Linear.test <- predict(Linear,newdata = data["x"])
  Quadratic.test <- predict(Quadratic,newdata = data["x"])
  Cubic.test <- predict(Cubic,newdata = data["x"])
  Quartic.test <- predict(Quartic,newdata = data["x"])
  Quintic.test <- predict(Quintic,newdata = data["x"])
  
  Linear.error <- mean((Linear.test-data$Y)^2)
  Quadratic.error <- mean((Quadratic.test-data$Y)^2)
  Cubic.error <- mean((Cubic.test-data$Y)^2)
  Quartic.error <- mean((Quartic.test-data$Y)^2)
  Quintic.error <- mean((Quintic.test-data$Y)^2)
  
  return(c(Linear.error,Quadratic.error,Cubic.error,
           Quartic.error,Quintic.error))
}

test.error(train)
```

8) [10 pts] Use the **Split/Apply/Combine** model to compute the test error for each test set: **TestSet1**, **TestSet2**, **TestSet3**, **TestSet4**. You can exhaustively perform this task for partial credit.  Display the test errors for each test set and each model.  Also display the average test error over each validation set.    

```{r}
# Solution goes here 
# 只是说Split/Apply/Combine，没有说其他的，就还是用第二种好啦
# apply()函数中，1是对列求值，2是对行求值


library(plyr)
five_error <- daply(data.test, .(ValSet), test.error)
five_error
apply(five_error,2,mean)

##--------------------------------------

data.split <- split(data.test, data.test$ValSet)
names(data.split)
five_error2 <- sapply(data.split,test.error)
five_error2
apply(five_error2, 1, mean)

```

9) [5 pts] Create a plot (base R or ggplot) showing both the **average test error** and the **training error** as a function of the polynomial's **degree**. Briefly comment on this plot.     

```{r}
# Solution goes here 
training_error <- test.error(train)
avg_test_error <- apply(five_error,2,mean)

plot(c(1:5),training_error,col="blue",type="l",xlab = "polynomial's degree",ylab = "error")
lines(c(1:5),avg_test_error,col="red")
legend("topright", legend = c("training_error","avg_test_error"),
       fill = c("blue","red"))

```



##### The linear degree=1 model does a poor job on both the training and test datasets. The test error achievesits minimum at degree 2. The training error continues to decrease as the degree increases. As the degreeincreases, we expect for the test error to increase and the training error to decrease.


